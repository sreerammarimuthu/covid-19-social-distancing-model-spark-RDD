{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cfb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from faker import Faker\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import csv\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Creating a SparkConf with memory configurations to handle the large dataset & SparkContext\n",
    "conf = SparkConf().setAppName(\"Covid\").set(\"spark.executor.memory\", \"4g\").set(\"spark.driver.memory\", \"4g\").set(\"spark.network.timeout\", \"600s\")\n",
    "spark = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "# Dataset Creation\n",
    "def generate_concert_data(name,size):\n",
    "    fake = Faker()\n",
    "    data = []\n",
    "    used_coordinates = set()\n",
    "\n",
    "    for id in range(1, size+1):\n",
    "        # Generate coordinates of the person X and Y values\n",
    "        x = round(random.uniform(1, 10000))\n",
    "        y = round(random.uniform(1, 10000))\n",
    "\n",
    "        # Ensure uniqueness by checking and regenerating if necessary\n",
    "        while (x, y) in used_coordinates:\n",
    "            x = round(random.uniform(1, 10000))\n",
    "            y = round(random.uniform(1, 10000))\n",
    "\n",
    "        # Add the coordinates to the used set\n",
    "        used_coordinates.add((x, y))\n",
    "\n",
    "        data.append({\n",
    "            \"ID\": id,\n",
    "            \"X\": x,\n",
    "            \"Y\": y,\n",
    "            \"Name\": fake.name(),\n",
    "            \"Age\": random.randint(10, 100)\n",
    "        })        \n",
    "    pd.DataFrame(data).to_csv(f\"{name}.csv\", index=False)\n",
    "generate_concert_data('PEOPLE_large', 100000)\n",
    "\n",
    "\n",
    "people_large=pd.read_csv('PEOPLE_large.csv')\n",
    "\n",
    "infected_small=people_large.sample(5000).reset_index(drop=True)\n",
    "infected_small.to_csv('INFECTED_small.csv',index=False)\n",
    "\n",
    "some_infected=pd.read_csv('PEOPLE_large.csv')\n",
    "some_infected['INFECTED']=['Yes' if i in list(infected_small['ID']) else 'No' for i in some_infected['ID']]\n",
    "some_infected=some_infected.reset_index(drop=True)\n",
    "some_infected.to_csv('SOME_INFECTED_large.csv')\n",
    "\n",
    "# Storing the data as RDDs\n",
    "infected_rdd = spark.textFile(\"INFECTED_small.csv\")\n",
    "person_rdd=spark.textFile(\"PEOPLE_large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f0dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query-1: People from people that had close contact with an infected person from Infected\n",
    "\n",
    "def csv_(line):\n",
    "    return line.split(',')\n",
    "\n",
    "# Calculating the distance between 2 people\n",
    "def distance_between(point1, point2):\n",
    "    x1, y1 = point1[1], point1[2]\n",
    "    x2, y2 = point2[1], point2[2]\n",
    "    return  (((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5)\n",
    "\n",
    "infected_q1 = infected_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "people_q1 = person_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "\n",
    "infected_coordinates = infected_q1.map(lambda x: (x[0], float(x[1]), float(x[2]),x[3],x[4]))\n",
    "people_coordinates = people_q1.map(lambda x: ((x[0]), float(x[1]), float(x[2]),x[3],x[4]))\n",
    "\n",
    "infected_id=infected_coordinates.map(lambda x:x[0]).collect()\n",
    "people_coordinates=people_coordinates.filter(lambda x:x[0] not in infected_id)\n",
    "\n",
    "close_contacts=infected_coordinates.cartesian(people_coordinates)\n",
    "close_contacts=close_contacts.filter(lambda pair: distance_between(pair[0], pair[1]) <= 6) \\\n",
    "    .map(lambda pair: (pair[1][0], pair[0][0]))\n",
    "\n",
    "# print(close_contacts.collect())\n",
    "\n",
    "result_list = close_contacts.collect()\n",
    "output_file_path_q1 = \"Query-1.txt\"\n",
    "\n",
    "with open(output_file_path_q1, \"w\") as file:\n",
    "    for pair in result_list:\n",
    "        file.write(f\"{pair[0]}, {pair[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c67fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query-2: Distinct people from  Query-1 case \n",
    "\n",
    "infected_q2 = infected_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "people_q2 = person_rdd.map(csv_).filter(lambda x: x[0] != 'ID')\n",
    "\n",
    "infected_coordinates = infected_q2.map(lambda x: (x[0], float(x[1]), float(x[2]), x[3], x[4]))\n",
    "people_coordinates = people_q2.map(lambda x: (x[0], float(x[1]), float(x[2]), x[3], x[4]))\n",
    "\n",
    "infected_id = infected_coordinates.map(lambda x: x[0]).collect()\n",
    "people_coordinates = people_coordinates.filter(lambda x: x[0] not in infected_id)\n",
    "\n",
    "distinct_close_contacts = infected_coordinates.cartesian(people_coordinates)\n",
    "\n",
    "distinct_close_contacts = distinct_close_contacts.filter(lambda pair: distance_between(pair[0], pair[1]) <= 6) \\\n",
    "    .map(lambda pair: (pair[1][0], pair[0][0])) \\\n",
    "    .map(lambda x: (x[0], x)) \\\n",
    "    .reduceByKey(lambda x, y: x) \\\n",
    "    .map(lambda x: x[1])\n",
    "\n",
    "# print(distinct_close_contacts.collect())\n",
    "\n",
    "distinct_result_list = distinct_close_contacts.collect()\n",
    "output_file_path_q2 = \"Query-2.txt\"\n",
    "\n",
    "with open(output_file_path_q2, \"w\") as file:\n",
    "    for pair in distinct_result_list:\n",
    "        file.write(f\"{pair[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query-3: Infected People with number of people they were in close contact with\n",
    "\n",
    "some_infected_rdd = spark.textFile(\"SOME_INFECTED_large.csv\")\n",
    "some_infected_rdd = some_infected_rdd.map(csv_).filter(lambda x: x[0] != 'ID').filter(lambda x: x[0] != '')\n",
    "some_infected_rdd = some_infected_rdd.map(lambda x: (x[1], float(x[2]), float(x[3]),x[4],x[5],x[6]))\n",
    "\n",
    "people_infected_yes=some_infected_rdd.filter(lambda x:x[5]=='Yes')\n",
    "people_infected_no=some_infected_rdd.filter(lambda x:x[5]=='No')\n",
    "\n",
    "close_contacts_cross=people_infected_yes.cartesian(people_infected_no)\n",
    "\n",
    "close_contacts_cross=close_contacts_cross.filter(lambda pair: distance_between(pair[0], pair[1]) <= 6) \\\n",
    "    .map(lambda pair: (pair[1][0],pair[1][3], pair[0][0],pair[0][3]))\n",
    "\n",
    "infected_counts = close_contacts_cross.map(lambda x: (x[2], 1)).reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "# print(infected_counts)\n",
    "\n",
    "result_count = infected_counts\n",
    "output_file_path_q3 = \"Query-3.txt\"\n",
    "\n",
    "with open(output_file_path_q3, \"w\") as file:\n",
    "    for pair in result_count:\n",
    "        file.write(f\"{pair[0]}, {pair[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7252d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
